import requests
import os
import re
from urllib.parse import unquote
import time

# ====================================================================
# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ø³Ú©Ø±ÛŒÙ¾Øª
# ====================================================================
SEARCH_KEYWORDS = [
    "v2ray config", "vless sub", "vmess subscription", "ss sub",
    "trojan subscription", "all_configs.txt", "sub_merge.txt"
]
EXISTING_SOURCES_FILE = "merge_configs.py"
OUTPUT_FILE = "discovered_sources.txt"
GITHUB_TOKEN = os.getenv("GH_PAT")

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¬Ø¯ÛŒØ¯ ---
MAX_DEPTH = 10  # Ø­Ø¯Ø§Ú©Ø«Ø± Ø¹Ù…Ù‚ Ø¨Ø±Ø§ÛŒ Ø¯Ù†Ø¨Ø§Ù„ Ú©Ø±Ø¯Ù† Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ ØªÙˆØ¯Ø±ØªÙˆ
REQUEST_TIMEOUT = 10  # Ø²Ù…Ø§Ù† Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø¯Ø±Ø®ÙˆØ§Ø³Øª (Ø¨Ù‡ Ø«Ø§Ù†ÛŒÙ‡)

# Ø¹Ø¨Ø§Ø±Øª Ù…Ù†Ø¸Ù… Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† URLÙ‡Ø§ÛŒ http/https
URL_REGEX = re.compile(r'https?://[^\s"\'`<>]+')

# Ø¹Ø¨Ø§Ø±Øª Ù…Ù†Ø¸Ù… Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ù¾Ø±ÙˆØªÚ©Ù„â€ŒÙ‡Ø§ÛŒ Ù¾Ø±ÙˆÚ©Ø³ÛŒ
PROXY_PROTOCOL_REGEX = re.compile(r'^(vmess|vless|ss|ssr|trojan|hysteria|hysteria2|tuic|brook)://')

# ====================================================================

def get_existing_urls(file_path):
    """Ù„ÛŒØ³Øª URLÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø±Ø§ Ø§Ø² ÙØ§ÛŒÙ„ merge_configs.py Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒÚ©Ù†Ø¯"""
    urls = set()
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()
            found_urls = re.findall(r'https?://[^\s"]+', content)
            for url in found_urls:
                urls.add(url.strip(',"\''))
    except FileNotFoundError:
        print(f"Warning: File {file_path} not found.")
    return urls

def search_github(query, token):
    """ÛŒÚ© Ú©ÙˆØ¦Ø±ÛŒ Ø±Ø§ Ø¯Ø± API Ø¬Ø³ØªØ¬ÙˆÛŒ Ú©Ø¯ Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨ Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒÚ©Ù†Ø¯"""
    headers = {"Accept": "application/vnd.github.v3+json"}
    if token:
        headers["Authorization"] = f"token {token}"
    search_query = f'{query} extension:txt'
    params = {"q": search_query, "per_page": 100}
    try:
        response = requests.get("https://api.github.com/search/code", headers=headers, params=params, timeout=20)
        response.raise_for_status()
        return response.json().get("items", [])
    except requests.RequestException as e:
        print(f"Error during GitHub API request: {e}")
        return []

def process_url_recursively(url, final_sources, visited_urls, depth):
    """ÛŒÚ© URL Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¨Ø§Ø²Ú¯Ø´ØªÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ø±Ø¯Ù‡ Ùˆ Ù…Ù†Ø§Ø¨Ø¹ Ù†Ù‡Ø§ÛŒÛŒ Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ù…ÛŒâ€ŒÚ©Ù†Ø¯"""
    if depth > MAX_DEPTH or url in visited_urls:
        return

    # Ù†Ù…Ø§ÛŒØ´ ÙˆØ¶Ø¹ÛŒØª Ù¾Ø±Ø¯Ø§Ø²Ø´
    indent = "  " * depth
    print(f"{indent}Processing URL (Depth {depth}): {url}")
    visited_urls.add(url)

    try:
        response = requests.get(url, timeout=REQUEST_TIMEOUT)
        response.raise_for_status()
        content = response.text
    except requests.RequestException:
        return

    lines = content.splitlines()
    is_direct_config_source = False
    potential_new_urls = []

    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        # Ø§Ú¯Ø± Ø®Ø· ÛŒÚ© Ú©Ø§Ù†ÙÛŒÚ¯ Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨ÙˆØ¯ØŒ Ø§ÛŒÙ† URL ÛŒÚ© Ù…Ù†Ø¨Ø¹ Ù†Ù‡Ø§ÛŒÛŒ Ø§Ø³Øª
        if PROXY_PROTOCOL_REGEX.match(line):
            is_direct_config_source = True
            break
        
        # Ø¯Ø± ØºÛŒØ± Ø§ÛŒÙ† ØµÙˆØ±ØªØŒ Ø¨Ù‡ Ø¯Ù†Ø¨Ø§Ù„ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø± Ø¯Ø± Ø§ÛŒÙ† Ø®Ø· Ø¨Ú¯Ø±Ø¯
        found_urls_in_line = URL_REGEX.findall(line)
        for found_url in found_urls_in_line:
            # ÙÙ‚Ø· Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒÛŒ Ø±Ø§ Ø¯Ø± Ù†Ø¸Ø± Ø¨Ú¯ÛŒØ± Ú©Ù‡ Ø¨Ù‡ Ù†Ø¸Ø± Ù„ÛŒÙ†Ú© Ø§Ø´ØªØ±Ø§Ú© Ù‡Ø³ØªÙ†Ø¯
            # (Ù…Ø«Ù„Ø§Ù‹ Ø¨Ù‡ .txt Ø®ØªÙ… Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ ÛŒØ§ Ù¾Ø³ÙˆÙ†Ø¯ Ø®Ø§ØµÛŒ Ù†Ø¯Ø§Ø±Ù†Ø¯)
            path = os.path.basename(unquote(found_url.split('?')[0]))
            if path.endswith('.txt') or '.' not in path:
                 potential_new_urls.append(found_url)

    if is_direct_config_source:
        print(f"{indent}  -> âœ… Found direct configs. Adding parent URL to final sources.")
        final_sources.add(url)
    else:
        # Ø§Ú¯Ø± Ú©Ø§Ù†ÙÛŒÚ¯ Ù…Ø³ØªÙ‚ÛŒÙ…ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ØŒ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø±Ø§ Ø¯Ù†Ø¨Ø§Ù„ Ú©Ù†
        print(f"{indent}  -> ğŸ“„ Not a direct config source. Found {len(potential_new_urls)} potential new URLs to crawl.")
        for new_url in set(potential_new_urls): # set() for uniqueness in this level
            time.sleep(0.1)  # ÙØ§ØµÙ„Ù‡ Ú©ÙˆØªØ§Ù‡ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ÙØ´Ø§Ø± Ø¨Ù‡ Ø³Ø±ÙˆØ±
            process_url_recursively(new_url, final_sources, visited_urls, depth + 1)

def main():
    if not GITHUB_TOKEN:
        print("âŒ ERROR: GH_PAT environment variable is not set.")
        return

    print("1. Getting existing URLs...")
    existing_urls = get_existing_urls(EXISTING_SOURCES_FILE)
    
    final_sources = set()
    visited_urls = set(existing_urls) # Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø±Ø§ Ø§Ø² Ø§Ø¨ØªØ¯Ø§ Ø¨Ù‡ Ù„ÛŒØ³Øª Ø¯ÛŒØ¯Ù‡â€ŒØ´Ø¯Ù‡ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†

    print("\n2. Searching GitHub for initial seed URLs...")
    initial_seed_urls = set()
    for keyword in SEARCH_KEYWORDS:
        print(f"   - Searching for: '{keyword}'")
        results = search_github(keyword, GITHUB_TOKEN)
        for item in results:
            raw_url = item.get("html_url").replace("github.com", "raw.githubusercontent.com").replace("/blob/", "/")
            initial_seed_urls.add(unquote(raw_url))
    
    print(f"\n3. Starting deep crawl from {len(initial_seed_urls)} seed URLs...")
    for url in initial_seed_urls:
        process_url_recursively(url, final_sources, visited_urls, depth=1)

    # ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø§Ø² Ù‚Ø¨Ù„ Ø¯Ø§Ø´ØªÛŒØ¯
    new_final_sources = final_sources - existing_urls

    if not new_final_sources:
        print("\nâœ… No new sources found after deep crawl.")
        return

    print(f"\n4. Discovered {len(new_final_sources)} new final source URLs!")
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        for url in sorted(list(new_final_sources)):
            f.write(url + "\n")

    print(f"\nâœ… Successfully saved new sources to '{OUTPUT_FILE}'.")

if __name__ == "__main__":
    main()
